{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:07:53.970265Z",
     "start_time": "2025-09-21T12:07:52.747633Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:07:54.474843Z",
     "start_time": "2025-09-21T12:07:54.382205Z"
    }
   },
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64               NaN  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                NaN  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  salary  \n",
       "0      United-States   <=50K  \n",
       "1      United-States   <=50K  \n",
       "2      United-States   <=50K  \n",
       "3      United-States   <=50K  \n",
       "4               Cuba   <=50K  \n",
       "...              ...     ...  \n",
       "48837  United-States  <=50K.  \n",
       "48838  United-States  <=50K.  \n",
       "48839  United-States  <=50K.  \n",
       "48840  United-States  <=50K.  \n",
       "48841  United-States   >50K.  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:07:54.701671Z",
     "start_time": "2025-09-21T12:07:54.557738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = data.dropna().copy()\n",
    "\n",
    "df['salary'] = df['salary'].astype(str).apply(lambda x: 1 if '>' in x else 0)\n",
    "\n",
    "categorical_cols = [\n",
    "    'marital-status', 'workclass', 'education', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "continuous_cols = [x for x in df.columns if x not in categorical_cols and x != 'salary']\n",
    "scaler = StandardScaler()\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", categorical_transformer, categorical_cols),\n",
    "        (\"continuous\", scaler, continuous_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Full pipeline: preprocessing + model\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor)\n",
    "\n",
    "])\n",
    "df_preprocessed = preprocessor.fit_transform(df)\n",
    "# Get transformed data\n",
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Convert to dense DataFrame\n",
    "df_preprocessed = pd.DataFrame(\n",
    "    df_preprocessed.toarray(),  # make it dense\n",
    "    columns=feature_names\n",
    ")\n",
    "df_preprocessed.head()\n",
    "\n",
    "# Fit pipeline\n",
    "# model.fit(X, y)\n",
    "#\n",
    "# # Predict\n",
    "# preds = model.predict(X)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   marital-status_Divorced  marital-status_Married-AF-spouse  \\\n",
       "0                      0.0                               0.0   \n",
       "1                      0.0                               0.0   \n",
       "2                      1.0                               0.0   \n",
       "3                      0.0                               0.0   \n",
       "4                      0.0                               0.0   \n",
       "\n",
       "   marital-status_Married-civ-spouse  marital-status_Married-spouse-absent  \\\n",
       "0                                0.0                                   0.0   \n",
       "1                                1.0                                   0.0   \n",
       "2                                0.0                                   0.0   \n",
       "3                                1.0                                   0.0   \n",
       "4                                1.0                                   0.0   \n",
       "\n",
       "   marital-status_Never-married  marital-status_Separated  \\\n",
       "0                           1.0                       0.0   \n",
       "1                           0.0                       0.0   \n",
       "2                           0.0                       0.0   \n",
       "3                           0.0                       0.0   \n",
       "4                           0.0                       0.0   \n",
       "\n",
       "   marital-status_Widowed  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0                     0.0                    0.0                  0.0   \n",
       "1                     0.0                    0.0                  0.0   \n",
       "2                     0.0                    0.0                  0.0   \n",
       "3                     0.0                    0.0                  0.0   \n",
       "4                     0.0                    0.0                  0.0   \n",
       "\n",
       "   workclass_Private  ...  native-country_United-States  \\\n",
       "0                0.0  ...                           1.0   \n",
       "1                0.0  ...                           1.0   \n",
       "2                1.0  ...                           1.0   \n",
       "3                1.0  ...                           1.0   \n",
       "4                1.0  ...                           0.0   \n",
       "\n",
       "   native-country_Vietnam  native-country_Yugoslavia       age    fnlwgt  \\\n",
       "0                     0.0                        0.0  0.034201 -1.062295   \n",
       "1                     0.0                        0.0  0.866417 -1.007438   \n",
       "2                     0.0                        0.0 -0.041455  0.245284   \n",
       "3                     0.0                        0.0  1.093385  0.425853   \n",
       "4                     0.0                        0.0 -0.798015  1.407393   \n",
       "\n",
       "   education-num  capital-gain  capital-loss  hours-per-week  salary  \n",
       "0       1.128753      0.142888      -0.21878       -0.078120     0.0  \n",
       "1       1.128753     -0.146733      -0.21878       -2.326738     0.0  \n",
       "2      -0.438122     -0.146733      -0.21878       -0.078120     0.0  \n",
       "3      -1.221559     -0.146733      -0.21878       -0.078120     0.0  \n",
       "4       1.128753     -0.146733      -0.21878       -0.078120     0.0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marital-status_Divorced</th>\n",
       "      <th>marital-status_Married-AF-spouse</th>\n",
       "      <th>marital-status_Married-civ-spouse</th>\n",
       "      <th>marital-status_Married-spouse-absent</th>\n",
       "      <th>marital-status_Never-married</th>\n",
       "      <th>marital-status_Separated</th>\n",
       "      <th>marital-status_Widowed</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034201</td>\n",
       "      <td>-1.062295</td>\n",
       "      <td>1.128753</td>\n",
       "      <td>0.142888</td>\n",
       "      <td>-0.21878</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866417</td>\n",
       "      <td>-1.007438</td>\n",
       "      <td>1.128753</td>\n",
       "      <td>-0.146733</td>\n",
       "      <td>-0.21878</td>\n",
       "      <td>-2.326738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.041455</td>\n",
       "      <td>0.245284</td>\n",
       "      <td>-0.438122</td>\n",
       "      <td>-0.146733</td>\n",
       "      <td>-0.21878</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093385</td>\n",
       "      <td>0.425853</td>\n",
       "      <td>-1.221559</td>\n",
       "      <td>-0.146733</td>\n",
       "      <td>-0.21878</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.798015</td>\n",
       "      <td>1.407393</td>\n",
       "      <td>1.128753</td>\n",
       "      <td>-0.146733</td>\n",
       "      <td>-0.21878</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable. \n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices \n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf). \n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec). \n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:07:54.786978Z",
     "start_time": "2025-09-21T12:07:54.772642Z"
    }
   },
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Define your preprocessing steps here\n",
    "scaler = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "steps = [\n",
    "        (\"categorical\", categorical_transformer, categorical_cols),\n",
    "        (\"continuous\", scaler, continuous_cols)\n",
    "    ]\n",
    "\n",
    "# Combine steps into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=steps,\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "target_col = 'salary'\n",
    "data = df.copy()\n",
    "dataX = data.drop(columns=[target_col])\n",
    "dataY = data[target_col]\n",
    "# show the correlation between different features including target variable\n",
    "def visualize(data, preprocessor):\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Fit + transform features\n",
    "    X_trans = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Convert to DataFrame (handle sparse matrices)\n",
    "    if hasattr(X_trans, \"toarray\"):\n",
    "        X_df = pd.DataFrame(X_trans.toarray(), columns=feature_names)\n",
    "    else:\n",
    "        X_df = pd.DataFrame(X_trans, columns=feature_names)\n",
    "\n",
    "    # Add target variable\n",
    "    X_df[target_col] = y.values\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr = X_df.corr(numeric_only=True)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Correlation Matrix Including Target Variable\")\n",
    "    plt.show()\n",
    "\n",
    "    return corr\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, X, y):    \n",
    "    # Wrap the model and steps into a Pipeline\n",
    "    pipeline = Pipeline(steps=[('t', preprocessor), ('m', model)])\n",
    "    \n",
    "    # Evaluate the model and store results\n",
    "    return evaluate_model(X, y, pipeline)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline):\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring = 'accuracy')\n",
    "    # cross validation for more reliable estimates\n",
    "    results = {\n",
    "            \"Accuracy\": scores.mean(),\n",
    "        }\n",
    "    print(\"Accuracy: \")\n",
    "    print(results)\n",
    "    print()\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:15:19.582601Z",
     "start_time": "2025-09-21T12:12:04.754237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models = [LogisticRegression(), LinearSVC(), SVC(), GradientBoostingClassifier(random_state=42)]\n",
    "dataY = dataY.astype(int)\n",
    "for m in models:\n",
    "    print(f'Model {m.__class__.__name__}:')\n",
    "    apply_model(m, dataX, dataY)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression:\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Model LinearSVC:\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Model SVC:\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8533457729779526)}\n",
      "\n",
      "Model GradientBoostingClassifier:\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:07:54.982539Z",
     "start_time": "2025-09-21T12:07:54.979630Z"
    }
   },
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    y_copy = y.copy()\n",
    "    n_flip = int(len(y) * fraction)\n",
    "\n",
    "    # Randomly select indices to flip\n",
    "    flip_indices = np.random.choice(len(y), size=n_flip, replace=False)\n",
    "\n",
    "    # Flip the labels (binary case)\n",
    "    y_copy[flip_indices] = 1 ^ y_copy[flip_indices]\n",
    "\n",
    "    return y_copy"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot. \n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T12:38:06.995698Z",
     "start_time": "2025-09-21T12:26:06.575585Z"
    }
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "fractions = np.arange(0, 0.6, 0.1)  # 0,0.1,...,0.5\n",
    "n_repeats = 5\n",
    "X = data.drop(columns=[target_col])\n",
    "y = data[target_col]\n",
    "pert_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Linear SVC\": LinearSVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {model_name: [] for model_name in pert_models.keys()}\n",
    "\n",
    "# -------------------------------\n",
    "# Run experiments\n",
    "# -------------------------------\n",
    "for frac in fractions:\n",
    "    frac_results = {model_name: [] for model_name in pert_models.keys()}\n",
    "    print(f\"Fraction: {frac}\")\n",
    "    for repeat in range(n_repeats):\n",
    "        y_perturbed = pertubate(dataY.values, frac)\n",
    "        for model_name, model in pert_models.items():\n",
    "            score = apply_model(model, dataX, y_perturbed)  # uses your pipeline + cross-val\n",
    "            frac_results[model_name].append(score[\"Accuracy\"])\n",
    "    # Compute mean and variance per model\n",
    "    for model_name in pert_models.keys():\n",
    "        mean_score = np.mean(frac_results[model_name])\n",
    "        var_score = np.var(frac_results[model_name])\n",
    "        results[model_name].append((mean_score, var_score))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction: 0.0\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8445226469744744)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8445226469744744)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8445226469744744)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8445226469744744)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8485693552973252)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8488347050437527)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8445226469744744)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.8629208373582479)}\n",
      "\n",
      "Fraction: 0.1\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7749326295936896)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7714166354330857)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7732962539995487)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7866083411631214)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7742692723419122)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7712176720212394)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7771439769064648)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7873159877549694)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7753527218690567)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7715049989621405)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7745567386420398)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7878024394710315)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7778293920343733)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7742470995555419)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7749767037826739)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7889302492403087)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.776369968429023)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7722127311254431)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7770776248004658)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7888196297983276)}\n",
      "\n",
      "Fraction: 0.2\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.704015790133807)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7026226844058738)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7087479380335533)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7148512878137202)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7051878012268012)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7039494306931118)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7093892251847059)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7165539983995692)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7023794267641594)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7005661578587968)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7054088567433606)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7150502438908705)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7043253338692552)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7026447007187269)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7079960412200142)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.714851072662634)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.700300879014432)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6989077659518027)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7039495284890599)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.7122640886676745)}\n",
      "\n",
      "Fraction: 0.30000000000000004\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.635664172625496)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6355093567495953)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6367475145770973)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6418999381196138)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.633806805082162)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6330328552822893)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6363938868730699)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6408829629434039)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6362169837822534)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6355535800774006)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6346910295932985)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6424086286343114)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6339172093730571)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.633651874296022)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6356861864934504)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.642497102183808)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6340277163496978)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6329662855803052)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6330326132373172)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.6403520678586525)}\n",
      "\n",
      "Fraction: 0.4\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5659635700313117)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5661625921207272)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5658751233757011)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5692142560087665)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5629340593704533)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5628013820523409)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5648135997001577)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5670028035653469)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5670249934660083)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5671355835692049)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5696343556188295)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5701429605620725)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5640617566743901)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5639512056895726)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5635973188262824)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5662288928838534)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5649683251148063)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5648356282375043)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5625137348297091)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5686832900219527)}\n",
      "\n",
      "Fraction: 0.5\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.4984740727049308)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.4988721388878498)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5026978161431284)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5030736288580197)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5010612889652675)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.50048636125481)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5004421697106878)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5040908118506195)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5033611978438927)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5034717170450269)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5022776162922182)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.503847632445664)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5024988282822948)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5020123447825495)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.4992703468748938)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5009731112485144)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.49973483851077305)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5009510362580922)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.49739049604305363)}\n",
      "\n",
      "Accuracy: \n",
      "{'Accuracy': np.float64(0.5006634746069153)}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Observations + explanations: max. 400 words"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion. \n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Discovery\n",
    "\n",
    "You will be given a set of datasets and you are tasked to perform data discovery on the data sets.\n",
    "\n",
    "<b>The datasets are provided in the group lockers on brightspace. Let me know if you are having trouble accessing the datasets</b>\n",
    "\n",
    "The process is to have the goal of finding datasets that are related to each other, finding relationships between the datasets.\n",
    "\n",
    "The relationships that we are primarily working with are Join and Union relationships.\n",
    "\n",
    "So please implement two methods for allowing us to find those pesky Join and Union relationships.\n",
    "\n",
    "Try to do this with the datasets as is and no processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T15:50:31.590145Z",
     "start_time": "2025-09-22T15:50:31.567669Z"
    }
   },
   "source": [
    "import difflib\n",
    "import os\n",
    "import pandas as pd\n",
    "def load_datasets(path, n=20):\n",
    "    datasets = {}\n",
    "    for i in range(n):\n",
    "        file_path = os.path.join(path, f\"table_{i}.csv\")\n",
    "        print(i)\n",
    "        datasets[i] = pd.read_csv(file_path, on_bad_lines=\"skip\")\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# --- Union candidates ---\n",
    "def find_union_candidates(datasets):\n",
    "    unions = []\n",
    "    for name1, df1 in datasets.items():\n",
    "        for name2, df2 in datasets.items():\n",
    "            if name1 >= name2:\n",
    "                continue\n",
    "            if len(df1.columns) == len(df2.columns):\n",
    "                # Compare datatypes column by column\n",
    "                col_match = all(\n",
    "                    df1.dtypes.values[i] == df2.dtypes.values[i]\n",
    "                    for i in range(len(df1.columns))\n",
    "                )\n",
    "                if col_match:\n",
    "                    unions.append((name1, name2))\n",
    "    return unions\n",
    "\n",
    "\n",
    "# --- Join candidates ---\n",
    "def find_join_candidates(datasets, sample_size=100):\n",
    "    joins = []\n",
    "    for name1, df1 in datasets.items():\n",
    "        for name2, df2 in datasets.items():\n",
    "            if name1 >= name2:\n",
    "                continue\n",
    "            for col1 in df1.columns:\n",
    "                for col2 in df2.columns:\n",
    "                    # Check column name similarity\n",
    "                    if col1.lower() == col2.lower() or \\\n",
    "                       difflib.SequenceMatcher(None, col1.lower(), col2.lower()).ratio() > 0.8:\n",
    "                        joins.append((name1, col1, name2, col2))\n",
    "                    else:\n",
    "                        # Check value overlap (sampled for speed)\n",
    "                        try:\n",
    "                            vals1 = set(df1[col1].dropna().astype(str).sample(min(sample_size, len(df1))))\n",
    "                            vals2 = set(df2[col2].dropna().astype(str).sample(min(sample_size, len(df2))))\n",
    "                            if len(vals1 & vals2) > 0:\n",
    "                                joins.append((name1, col1, name2, col2))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "    return joins\n",
    "unionss = []\n",
    "joinss = []\n",
    "def discovery_algorithm():\n",
    "    \"\"\"Function should be able to perform data discovery to find related datasets\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of pairs of related datasets\n",
    "    \"\"\"\n",
    "    unionss = find_union_candidates(load_datasets(f'lake49/'))\n",
    "    joinss = find_join_candidates(load_datasets(f'lake49/'))\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-22T15:50:41.001575Z"
    }
   },
   "source": [
    "discovery_algorithm()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\traia\\AppData\\Local\\Temp\\ipykernel_36748\\4236436868.py:9: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets[i] = pd.read_csv(file_path, on_bad_lines=\"skip\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\traia\\AppData\\Local\\Temp\\ipykernel_36748\\4236436868.py:9: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  datasets[i] = pd.read_csv(file_path, on_bad_lines=\"skip\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "unionss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "joinss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would have noticed that the data has some issues in them.\n",
    "So perhaps those issues have been troublesome to deal with.\n",
    "\n",
    "Please try to do some cleaning on the data.\n",
    "\n",
    "After performing cleaning see if the results of the data discovery has changed?\n",
    "\n",
    "Please try to explain this in your report, and try to match up the error with the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning data, scrubbing, washing, mopping\n",
    "\n",
    "def cleaningData(data):\n",
    "    \"\"\"Function should be able to clean the data\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of cleaned datasets\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
