{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 Data Discovery\n",
    "\n",
    "You will be given a set of datasets and you are tasked to perform data discovery on the data sets.\n",
    "\n",
    "<b>The datasets are provided in the group lockers on brightspace. Let me know if you are having trouble accessing the datasets</b>\n",
    "\n",
    "The process is to have the goal of finding datasets that are related to each other, finding relationships between the datasets.\n",
    "\n",
    "The relationships that we are primarily working with are Join and Union relationships.\n",
    "\n",
    "So please implement two methods for allowing us to find those pesky Join and Union relationships.\n",
    "\n",
    "Try to do this with the datasets as is and no processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INFO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### CSV Reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# This function returns a dict of all likely delimiters it is able to find in a line-string\n",
    "# A likely delimiter is any character that is not a-z, A-Z, 0-9 or ., ', \"\n",
    "########\n",
    "def find_candidate_delimiters( line ):\n",
    "    candidates = {}\n",
    "    line = line.rstrip(\"\\r\\n\")\n",
    "    in_substring = False\n",
    "    for character in line:\n",
    "        if character == '\"' or character == \"'\":\n",
    "            in_substring ^= True\n",
    "        if in_substring:\n",
    "            continue\n",
    "        if character.isalnum() or character == '.':\n",
    "            continue\n",
    "        if character not in candidates:\n",
    "            candidates[character] = np.array([])\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########\n",
    "# Given a list of strings named \"file\" and a list of candidate delimiters this function counts\n",
    "# how many times each delimiter appears in each line (ie. list element)\n",
    "########\n",
    "def delimiter_consistency( file, candidates ):\n",
    "    for line in file:\n",
    "        line = line.rstrip(\"\\r\\n\")\n",
    "\n",
    "        in_substring = False\n",
    "        appearances = { char: 0 for char in candidates }\n",
    "        for character in line:\n",
    "            if character == '\"' or character == \"'\":\n",
    "                in_substring ^= True\n",
    "            if in_substring:\n",
    "                continue\n",
    "            if character in candidates:\n",
    "                appearances[character] += 1\n",
    "\n",
    "        for char in appearances:\n",
    "            if appearances[char] > 0:\n",
    "                candidates[char] = np.append ( candidates[char], appearances[char] )\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "#\n",
    "# This function attempts to figure out which delimiter is the most likely for a given csv\n",
    "# To do this it calculates the SNR of each likely delimiter per line.\n",
    "# As specific files are highly incorrect a high epsilon value of 0.5 was chosen to combat\n",
    "# high variances of delimiters with very low means\n",
    "#\n",
    "# Example Issue:\n",
    "# Table 7 has two likely delimiters, '/', ','.\n",
    "# Without making any assumptions the best SNR is given for '/' due to 0 variance.\n",
    "# This character however has a mean of 1 which is a lot less than ','.\n",
    "# ',' has significant variance but a vastly more sensical mean.\n",
    "# We increase epsilon to ensure that this delimiter is correctly found.\n",
    "#\n",
    "########\n",
    "def find_delimiter( file_path, max_rows=1000 ):\n",
    "    delimiter = ''\n",
    "\n",
    "    with open( file_path, mode='r', encoding='utf-8' ) as file:\n",
    "        # Only use a single line for candidates\n",
    "        # a random line near the middle is unlikely to have specific formatting\n",
    "        file = file.readlines()\n",
    "        candidates_line = file[ len(file)//2 ]\n",
    "        candidates = find_candidate_delimiters( candidates_line )\n",
    "\n",
    "        if len( candidates ) == 0:\n",
    "            return delimiter\n",
    "\n",
    "        step = len(file)//max_rows\n",
    "        delimiters = delimiter_consistency( file=file[::step], candidates=candidates )\n",
    "\n",
    "    # A delimiter must appear on all lines of the csv\n",
    "    # Drop those that do not\n",
    "    keys = list(delimiters.keys())\n",
    "    for key in keys:\n",
    "        if delimiters[key].shape[0] < 0.99*max_rows:\n",
    "            delimiters.pop( key, None )\n",
    "            continue\n",
    "\n",
    "    keys = list(delimiters.keys())\n",
    "    epsilon = 0.5\n",
    "\n",
    "    first_key = keys[0]\n",
    "    delimiter = first_key\n",
    "    max_snr = delimiters[first_key].mean()/(delimiters[first_key].std()+epsilon)\n",
    "\n",
    "    for key in keys[1:]:\n",
    "        snr = delimiters[key].mean()/(delimiters[key].std()+epsilon)\n",
    "        if snr > max_snr:\n",
    "            delimiter = key\n",
    "            max_snr = snr\n",
    "\n",
    "    return delimiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Reads a csv with as few assumptions about it as possible.\n",
    "# This produces a very unclean dataframe to be used in part 1\n",
    "# Part two will perform extra cleaning on this same csv\n",
    "########\n",
    "\n",
    "def read_csv_dirty( filepath ):\n",
    "    delimiter = find_delimiter( filepath )\n",
    "\n",
    "    result = []\n",
    "    with open( filepath, 'r' ) as file:\n",
    "        reader = csv.reader( file, delimiter=delimiter )\n",
    "        for line in reader:\n",
    "            result.append( line )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set Similarity Measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# The well-known levenshtein similarity measure.\n",
    "# Can be used in place of jaccard if needed.\n",
    "########\n",
    "def levenshtein(x, y):\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return len(x) + len(y)\n",
    "\n",
    "    if x[0] == y[0]:\n",
    "        return levenshtein( x[1:], y[1:] )\n",
    "    return 1+levenshtein( x[1:], y[1:] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# The well-known jaccard set similarity measure.\n",
    "# We return zero when both sets are empty because even though it's normally undefined\n",
    "# we want to avoid over-filling our pairs of similar sets with useless pairs\n",
    "# as such if we set their similarity to zero they will never be treated as similar\n",
    "########\n",
    "def jaccard(x,y):\n",
    "    if len(x) == 0 and len(y)==0:\n",
    "        return 0\n",
    "    return len( x & y ) / len( x | y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Union Finding\n",
    "\n",
    "Possible **JOIN** relationship candidates are calculated later within our *Lazo* and *SilkMoth* implementations. This section implements methods that can use the aforementioned **JOIN** candidates to see if they are numerous enough to accomodate a **UNION** between our tables. *exists_mapping* will tell us whether we should treat a pair of tables as **UNION** candidates.\n",
    "\n",
    "All relationships that are not find to be **UNIONS** are classified as **JOIN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Given a dict that represents a graph adjacency matrix this method returns:\n",
    "# True - if there's a subgraph of our graph that is bipartite\n",
    "# False - if there's no bipartite subgraph\n",
    "# This will be helpful to discover unions\n",
    "########\n",
    "def bipartite_match( node, seen, match, adjacency ):\n",
    "    for node2 in adjacency[node]:\n",
    "        if node2 in seen:\n",
    "            continue\n",
    "        seen.add( node2 )\n",
    "        if node2 not in match or bipartite_match( match[node2], seen, match, adjacency ):\n",
    "            match[node2] = node\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# This function uses the bipartite_match function to see if the graph\n",
    "# created by joining table columns depending on their similarity can\n",
    "# match all columns in both table one to one.\n",
    "# A small threshold is allowed in case a pair of columns was mistakenly\n",
    "# not found to be similar earlier ( a non-zero probability phenomenon )\n",
    "# If a graph matching exists we have a union candidate pair\n",
    "########\n",
    "\n",
    "def exists_mapping( pairs, threshold=0.9 ):\n",
    "    adjacency = {}\n",
    "    cols1 = set()\n",
    "    cols2 = set()\n",
    "    for p in pairs:\n",
    "        c1, c2 = p['col1'], p['col2']\n",
    "        if not c1 in adjacency:\n",
    "            adjacency[c1] = []\n",
    "        adjacency[c1].append(c2)\n",
    "        cols1.add(c1)\n",
    "        cols2.add(c2)\n",
    "\n",
    "    matches = 0\n",
    "    for node in adjacency:\n",
    "        if bipartite_match( node, set(), {}, adjacency ):\n",
    "            matches += 1\n",
    "\n",
    "    return matches >= threshold*len( cols1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lazo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Shingles a string to a list of k-shingles\n",
    "# Needed to later generate Lazo signatures\n",
    "########\n",
    "\n",
    "def k_shingle( doc, k=2 ):\n",
    "    if len( doc ) < k:\n",
    "        return set( [ doc ] )\n",
    "    shingles = set()\n",
    "    for i in range( len(doc) + 1 - k ):\n",
    "        shingles.add( doc[i:i+k] )\n",
    "    return shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Produces shingles for an entier dataframe, column by column\n",
    "########\n",
    "\n",
    "def dframe_shingles( dframe, k=2 ):\n",
    "    shingles = {}\n",
    "    for column in dframe.columns:\n",
    "        shingles[column] = set([])\n",
    "        for datum in dframe[column]:\n",
    "            shingles[column].update( k_shingle( str(datum), k=k ) )\n",
    "    return shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Produces shingles for all dataframes\n",
    "# Utilizes caching to avoid re-shingling on later executions\n",
    "# since it is very time consuming\n",
    "########\n",
    "def shingle_dframes( dframes, shingle_size=2 ):\n",
    "    if not os.path.exists( Path(\"./cache\") ):\n",
    "        os.mkdir( Path(\"./cache\") )\n",
    "\n",
    "    vocab_file = f\"./cache/vocab.{shingle_size}.pickle\"\n",
    "    vocab_loaded = False\n",
    "    if os.path.exists( vocab_file ):\n",
    "        with open( vocab_file, 'rb' ) as file:\n",
    "            vocab = pickle.load( file )\n",
    "        vocab_loaded = True\n",
    "    else:\n",
    "        vocab = set([])\n",
    "\n",
    "    shingles = {}\n",
    "    for title, dframe in dframes.items():\n",
    "        cache_file = f\"./cache/{title}.{shingle_size}.pickle\"\n",
    "        if os.path.exists( cache_file ):\n",
    "            with open( cache_file, 'rb' ) as file:\n",
    "                shingles[title] = pickle.load( file )\n",
    "        else:\n",
    "            shingles[title] = dframe_shingles( dframe, k=shingle_size )\n",
    "            with open( cache_file, 'wb' ) as file:\n",
    "                pickle.dump( shingles[title], file, pickle.HIGHEST_PROTOCOL )\n",
    "\n",
    "        if not vocab_loaded:\n",
    "            for _, col_shingles in shingles[title].items():\n",
    "                vocab.update( col_shingles )\n",
    "\n",
    "    if not vocab_loaded:\n",
    "        with open( vocab_file, 'wb' ) as file:\n",
    "            pickle.dump( vocab, file, pickle.HIGHEST_PROTOCOL )\n",
    "\n",
    "    return shingles, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Produces a one-hot list of all shingles given a vocabulary\n",
    "# To late be used for creating hashes and buckets\n",
    "########\n",
    "def onehot_shingles( shingles, vocab ):\n",
    "\n",
    "    onehot = { title: { col: [] for col in shingles[title] } for title in shingles }\n",
    "\n",
    "    for shingle in vocab:\n",
    "        for title in shingles:\n",
    "            for col in shingles[title]:\n",
    "                if shingle in shingles[title][col]:\n",
    "                    onehot[title][col].append( 1 )\n",
    "                else:\n",
    "                    onehot[title][col].append( 0 )\n",
    "\n",
    "    return onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# One-permutation hashing, the defining feature of Lazo\n",
    "# and a major speed increase over Min-Hash\n",
    "# Produces a signature for a given one-hot encoding of a document\n",
    "# in this case, a column\n",
    "# Also bands the signature to later be used for bucketing\n",
    "########\n",
    "def one_perm_hashing( onehot, vocab_length, num_hashes=100, n_bands=10 ):\n",
    "    assert num_hashes % n_bands == 0\n",
    "    assert num_hashes > n_bands\n",
    "    rows_per_band = num_hashes // n_bands\n",
    "    bin_size = vocab_length // num_hashes\n",
    "\n",
    "    idx = list( range( vocab_length ) )\n",
    "    shuffle( idx )\n",
    "\n",
    "    banded_sig = [ [ -1 for _ in range( rows_per_band ) ] for _ in range( n_bands ) ]\n",
    "    for b in range( num_hashes ):\n",
    "        start = b * bin_size\n",
    "        end = (b + 1) * bin_size\n",
    "        if b == num_hashes - 1:\n",
    "            end = vocab_length\n",
    "        bin_indices = idx[start:end]\n",
    "\n",
    "        band_id = b // rows_per_band\n",
    "        for i in bin_indices:\n",
    "            if onehot[i] == 1:\n",
    "                j = b % rows_per_band\n",
    "                banded_sig[band_id][j] = i\n",
    "                break\n",
    "\n",
    "    return banded_sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Creates a set of buckets using the signatures of table columns\n",
    "# Columns with similar signatures are placed in the same bucket\n",
    "########\n",
    "def hash_dframes( onehot, vocab_length, signature_length=100, num_bands=10 ):\n",
    "    buckets = [ {} for _ in range(num_bands) ]\n",
    "    for title in onehot:\n",
    "        for column in onehot[title]:\n",
    "            bands = one_perm_hashing( onehot[title][column], vocab_length, num_hashes=signature_length, n_bands=num_bands )\n",
    "\n",
    "            for band_idx, band in enumerate( bands ):\n",
    "                band_hash = hash( tuple(band) )\n",
    "                if band_hash not in buckets[band_idx]:\n",
    "                    buckets[band_idx][band_hash] = [ { 'title': title, 'column': column } ]\n",
    "                else:\n",
    "                    buckets[band_idx][band_hash].append( { 'title': title, 'column': column } )\n",
    "\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Full implementation of Lazo\n",
    "# After buckets are created the full similarities of\n",
    "# columns in the same bucket are calculated for greater\n",
    "# certainty. Minimum similarity thresholds apply.\n",
    "########\n",
    "def Lazo( documents, shingle_size=2, signature_length=100, num_bands=10, similarity_threshold = 0.7, union_threshold=0.9 ):\n",
    "    t_start_shingling = time()\n",
    "\n",
    "    shingles, vocab = shingle_dframes( documents, shingle_size=shingle_size  )\n",
    "    vocab_length = len(vocab)\n",
    "\n",
    "    t_end_shingles = time()\n",
    "    if INFO:\n",
    "        print( f\"Finished shingling ({1000*(t_end_shingles-t_start_shingling):.4f}ms)\" )\n",
    "\n",
    "    onehot = onehot_shingles( shingles, vocab )\n",
    "\n",
    "    t_end_onehot = time()\n",
    "    if INFO:\n",
    "        print( f\"Finished onehot ({1000*(t_end_onehot-t_end_shingles):.4f}ms)\" )\n",
    "\n",
    "    buckets = hash_dframes( onehot, vocab_length )\n",
    "\n",
    "    t_end_hashing = time()\n",
    "    if INFO:\n",
    "        print( f\"Finished hashing ({1000*(t_end_hashing-t_end_onehot):.4f}ms)\" )\n",
    "\n",
    "\n",
    "    similarities = {}\n",
    "    for band_idx, band_buckets in enumerate(buckets):\n",
    "        for key in band_buckets:\n",
    "            bucket = band_buckets[key]\n",
    "            if len( bucket ) < 2:\n",
    "                continue\n",
    "\n",
    "            for i in range( len(bucket) ):\n",
    "\n",
    "                doc1 = bucket[i]\n",
    "                title1 = doc1['title']\n",
    "                col1 = doc1['column']\n",
    "                for j in range( i ):\n",
    "                    doc2 = bucket[j]\n",
    "                    title2 = doc2['title']\n",
    "                    col2 = doc2['column']\n",
    "                    similarity = jaccard( shingles[title1][col1], shingles[title2][col2] )\n",
    "\n",
    "                    if similarity < similarity_threshold:\n",
    "                        continue\n",
    "\n",
    "                    if title1 not in similarities and title2 not in similarities:\n",
    "                        similarities[title1] = {}\n",
    "                        similarities[title1][title2] = [ { 'col1': col1, 'col2': col2, 'similarity': similarity } ]\n",
    "                    elif title1 in similarities and title2 in similarities[title1]:\n",
    "                        similarities[title1][title2].append( { 'col1': col1, 'col2': col2, 'similarity': similarity } )\n",
    "                    elif title2 in similarities and title1 in similarities[title2]:\n",
    "                        similarities[title2][title1].append( { 'col1': col2, 'col2': col1, 'similarity': similarity } )\n",
    "                    elif title1 in similarities:\n",
    "                        similarities[title1][title2] = [ { 'col1': col1, 'col2': col2, 'similarity': similarity } ]\n",
    "                    else:\n",
    "                        similarities[title2][title1] = [ { 'col1': col2, 'col2': col1, 'similarity': similarity } ]\n",
    "\n",
    "    candidates = { 'union': [], 'join': [] }\n",
    "    for table1 in similarities:\n",
    "        for table2 in similarities[table1]:\n",
    "            if table2==table1:\n",
    "                continue\n",
    "\n",
    "            if documents[table1].shape[1] != documents[table2].shape[1]:\n",
    "                candidates['join'].append( [table1, table2] )\n",
    "                continue\n",
    "\n",
    "            if exists_mapping( similarities[table1][table2] ):\n",
    "                candidates['union'].append( [table1, table2] )\n",
    "            else:\n",
    "                candidates['join'].append( [table1, table2] )\n",
    "\n",
    "    t_end_bucketing = time()\n",
    "    if INFO:\n",
    "        print( f\"Finished bucketing ({1000*(t_end_bucketing-t_end_hashing):.4f}ms)\" )\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SilkMoth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_string(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", str(s)).lower().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# This function returns the most likely datatype for a column\n",
    "# Subsamples rows to avoid long computation times\n",
    "########\n",
    "def detect_column_dtype( column, samples=1000 ):\n",
    "    vals = set( column )\n",
    "    step = len( vals ) // samples\n",
    "\n",
    "    if len( vals ) == 2:\n",
    "        return 'bool'\n",
    "\n",
    "    likely_dtypes = {\n",
    "        'int': 0,\n",
    "        'float': 0,\n",
    "        'datetime': 0,\n",
    "        'string': 1\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    for val in vals:\n",
    "        if count ==samples:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "        try:\n",
    "            tmp = pd.to_datetime( val )\n",
    "            likely_dtypes['datetime'] += 1\n",
    "            continue\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            tmp = int( val )\n",
    "            likely_dtypes['int'] += 1\n",
    "            continue\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            tmp = float( val )\n",
    "            likely_dtypes['float'] += 1\n",
    "            continue\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        likely_dtypes['string'] += 1\n",
    "\n",
    "    if likely_dtypes['float'] > 0.1*likely_dtypes['int']:\n",
    "        likely_dtypes['float'] += likely_dtypes['int']\n",
    "\n",
    "    dtype = 'string'\n",
    "    most_likely = likely_dtypes['string']\n",
    "    if likely_dtypes['float'] >= most_likely:\n",
    "        most_likely = likely_dtypes['float']\n",
    "        dtype = 'float'\n",
    "    if likely_dtypes['int'] >= most_likely:\n",
    "        most_likely = likely_dtypes['int']\n",
    "        dtype = 'int'\n",
    "    if likely_dtypes['datetime'] >= most_likely:\n",
    "        dtype = 'datetime'\n",
    "\n",
    "    return dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Creates a signature for the table given the detected data types\n",
    "# of its columns\n",
    "########\n",
    "def table_signature(df: pd.DataFrame):\n",
    "    sig = []\n",
    "    for col in df.columns:\n",
    "        sig.append(detect_column_dtype(df[col]))\n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def column_similarity(col1, col2, sample_size=10):\n",
    "    vals1 = col1.dropna().astype(str).map(normalize_string)\n",
    "    vals2 = col2.dropna().astype(str).map(normalize_string)\n",
    "\n",
    "    if sample_size and len(vals1) > sample_size:\n",
    "        vals1 = vals1.sample(sample_size, random_state=42)\n",
    "    if sample_size and len(vals2) > sample_size:\n",
    "        vals2 = vals2.sample(sample_size, random_state=42)\n",
    "\n",
    "    set1, set2 = set(vals1), set(vals2)\n",
    "    jaccard_sim = jaccard( set1, set2 )\n",
    "\n",
    "    name_sim = SequenceMatcher(None, normalize_string(col1.name), normalize_string(col2.name)).ratio()\n",
    "    return 0.5 * jaccard_sim + 0.5 * name_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Union check\n",
    "def unionable_by_type(sig1, sig2, threshold=0.8):\n",
    "    if not sig1 or not sig2:\n",
    "        return False\n",
    "    counter1, counter2 = Counter(sig1), Counter(sig2)\n",
    "    common_count = sum(min(counter1[t], counter2[t]) for t in counter1)\n",
    "    overlap_ratio = common_count / max(len(sig1), len(sig2))\n",
    "    return overlap_ratio >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Relationship finder\n",
    "def find_relationships(datasets, join_threshold=0.6, union_threshold=0.8):\n",
    "    relationships = {\"join\": [], \"union\": []}\n",
    "\n",
    "    signatures = { name: table_signature(df) for name, df in datasets.items() }\n",
    "    for (i, (name1, df1)), (j, (name2, df2)) in itertools.combinations(enumerate(datasets.items()), 2):\n",
    "        sig1 = signatures[name1]\n",
    "        sig2 = signatures[name2]\n",
    "\n",
    "        if unionable_by_type(sig1, sig2, threshold=union_threshold):\n",
    "            #print(f\"[UNION] {name1} <--> {name2} (overlap â‰¥ {union_threshold})\")\n",
    "            relationships[\"union\"].append({\n",
    "                \"table1\": name1,\n",
    "                \"table2\": name2,\n",
    "                \"signature1\": sig1,\n",
    "                \"signature2\": sig2\n",
    "            })\n",
    "\n",
    "        joinable_cols = []\n",
    "        found = False\n",
    "        # takes a lot of time, so I only picked a few columns for speed, accuracy should\n",
    "        # theoretically increase for a higher threshold and more columns\n",
    "        for i, col1 in enumerate( df1.columns[:10] ):\n",
    "            if found:\n",
    "                break\n",
    "            for j, col2 in enumerate( df2.columns[:10] ):\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "                dtype1 = sig1[i]\n",
    "                dtype2 = sig2[j]\n",
    "\n",
    "                if dtype1 != dtype2:\n",
    "                    continue\n",
    "\n",
    "                sim = column_similarity(df1[col1], df2[col2])\n",
    "                if sim >= join_threshold:\n",
    "                    #print(f\"[JOIN] {name1}.{col1} <--> {name2}.{col2} (dtype={dtype1}, sim={sim:.2f})\")\n",
    "                    joinable_cols.append({\n",
    "                        \"col1\": col1,\n",
    "                        \"col2\": col2,\n",
    "                        \"similarity\": sim,\n",
    "                        \"dtype\": dtype1\n",
    "                    })\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        if joinable_cols:\n",
    "            relationships[\"join\"].append({\n",
    "                \"table1\": name1,\n",
    "                \"table2\": name2,\n",
    "                \"joinable_columns\": joinable_cols\n",
    "            })\n",
    "\n",
    "    return relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Testing on Unclean Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished shingling (143.8913ms)\n",
      "Finished onehot (30315.9292ms)\n",
      "Finished hashing (67785.0680ms)\n",
      "Finished bucketing (32.2101ms)\n",
      "\n",
      "\n",
      "====== Lazo ======\n",
      "Join Candidates: 10\n",
      "table_8, table_9, JOIN\n",
      "table_8, table_1, JOIN\n",
      "table_8, table_14, JOIN\n",
      "table_8, table_0, JOIN\n",
      "table_9, table_1, JOIN\n",
      "table_9, table_14, JOIN\n",
      "table_9, table_0, JOIN\n",
      "table_1, table_14, JOIN\n",
      "table_0, table_14, JOIN\n",
      "table_6, table_7, JOIN\n",
      "\n",
      "Union Candidates: 1\n",
      "table_1, table_0, UNION\n",
      "\n",
      "====== SilkMoth ======\n",
      "Join Candidates: 32\n",
      "table_17, table_0, JOIN\n",
      "table_17, table_8, JOIN\n",
      "table_17, table_7, JOIN\n",
      "table_17, table_1, JOIN\n",
      "table_17, table_16, JOIN\n",
      "table_17, table_4, JOIN\n",
      "table_17, table_5, JOIN\n",
      "table_17, table_6, JOIN\n",
      "table_0, table_9, JOIN\n",
      "table_0, table_1, JOIN\n",
      "table_0, table_16, JOIN\n",
      "table_0, table_5, JOIN\n",
      "table_8, table_7, JOIN\n",
      "table_8, table_16, JOIN\n",
      "table_8, table_6, JOIN\n",
      "table_13, table_12, JOIN\n",
      "table_7, table_1, JOIN\n",
      "table_7, table_16, JOIN\n",
      "table_7, table_6, JOIN\n",
      "table_10, table_9, JOIN\n",
      "table_10, table_15, JOIN\n",
      "table_9, table_1, JOIN\n",
      "table_9, table_16, JOIN\n",
      "table_9, table_4, JOIN\n",
      "table_9, table_5, JOIN\n",
      "table_1, table_16, JOIN\n",
      "table_1, table_5, JOIN\n",
      "table_16, table_4, JOIN\n",
      "table_16, table_5, JOIN\n",
      "table_16, table_6, JOIN\n",
      "table_4, table_3, JOIN\n",
      "table_4, table_5, JOIN\n",
      "\n",
      "Union Candidates: 8\n",
      "table_17, table_16, UNION\n",
      "table_0, table_1, UNION\n",
      "table_8, table_5, UNION\n",
      "table_13, table_12, UNION\n",
      "table_7, table_5, UNION\n",
      "table_7, table_6, UNION\n",
      "table_4, table_5, UNION\n",
      "table_15, table_11, UNION\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# Dirty data is loaded and tested against our algorithms\n",
    "########\n",
    "file_names = [ f[:-4] for f in os.listdir( Path('./lake49') ) if os.path.isfile( os.path.join( './lake49', f ) ) ]\n",
    "\n",
    "swamps = {}\n",
    "for swamp_name in file_names:\n",
    "    file_path = f\"./lake49/{swamp_name}.csv\"\n",
    "    data = read_csv_dirty( file_path )\n",
    "    swamps[swamp_name] = pd.DataFrame( data )\n",
    "\n",
    "candidates = Lazo( swamps, shingle_size=5, signature_length=100, num_bands=25, similarity_threshold=0.6, union_threshold=0.8 )\n",
    "\n",
    "print( \"\\n\\n====== Lazo ======\" )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, UNION\" )\n",
    "\n",
    "print( \"\\n====== SilkMoth ======\" )\n",
    "candidates = find_relationships( swamps, join_threshold=0.7, union_threshold=0.8 )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, UNION\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 15)\n",
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
      "       'salary'],\n",
      "      dtype='object')\n",
      "Finished shingling (31.3239ms)\n",
      "Finished onehot (759.9332ms)\n",
      "Finished hashing (2975.6005ms)\n",
      "Finished bucketing (0.4699ms)\n",
      "\n",
      "\n",
      "====== Lazo ======\n",
      "Join Candidates: 0\n",
      "\n",
      "Union Candidates: 1\n",
      "test, train, UNION\n",
      "\n",
      "====== SilkMoth ======\n",
      "Join Candidates: 1\n",
      "train, test, JOIN\n",
      "\n",
      "Union Candidates: 1\n",
      "train, test, UNION\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Sanity check using adult dataset\n",
    "data = pd.read_csv( Path( \"./adult.csv\" ), header=0, na_values='?' )\n",
    "\n",
    "print( data.shape )\n",
    "print( data.columns )\n",
    "\n",
    "# Check for overly empty feature columns\n",
    "for column in data.columns:\n",
    "    nans = data[column].isna()\n",
    "    if nans.sum() >= 0.6*data.shape[0]:\n",
    "        print( f\"Dropping {column}\" )\n",
    "        data.drop( column )\n",
    "\n",
    "# Drop any rows with NaN\n",
    "# Try imputation\n",
    "data.dropna( inplace=True )\n",
    "\n",
    "\n",
    "# Capital loss and capital gain represent essentially the exact same information\n",
    "# Capital delta can represent both at the same time without increasing dimensionality\n",
    "data['capital-delta'] = data['capital-gain'] - data['capital-loss']\n",
    "\n",
    "\n",
    "# Education is the same as education-num if it were ordinally encoded.\n",
    "# Capital loss/gain already encoded with capital delta\n",
    "data.drop( columns=[ 'education', 'capital-gain', 'capital-loss' ], inplace=True )\n",
    "\n",
    "slicepoint = data.shape[0]//2\n",
    "train, test = data[:slicepoint], data[slicepoint:]\n",
    "\n",
    "documents = { 'train': train, 'test': test }\n",
    "\n",
    "candidates = Lazo( documents, shingle_size=5, signature_length=100, num_bands=25, similarity_threshold=0.5, union_threshold=0.8 )\n",
    "\n",
    "print( \"\\n\\n====== Lazo ======\" )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, UNION\" )\n",
    "\n",
    "\n",
    "print( \"\\n====== SilkMoth ======\" )\n",
    "candidates = find_relationships( documents, join_threshold=0.6, union_threshold=0.8 )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, UNION\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would have noticed that the data has some issues in them.\n",
    "So perhaps those issues have been troublesome to deal with.\n",
    "\n",
    "Please try to do some cleaning on the data.\n",
    "\n",
    "After performing cleaning see if the results of the data discovery has changed?\n",
    "\n",
    "Please try to explain this in your report, and try to match up the error with the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Attempts to find a header, if any, in our table.\n",
    "# Multiline assumed possible.\n",
    "# Returns the amount of lines in the header.\n",
    "########\n",
    "def find_header( column, dtype ):\n",
    "    counter = 0\n",
    "    for val in column:\n",
    "        if dtype == 'int':\n",
    "            try:\n",
    "                tmp = int( val )\n",
    "                break\n",
    "            except (ValueError, TypeError):\n",
    "                counter += 1\n",
    "\n",
    "        if dtype == 'float':\n",
    "            try:\n",
    "                tmp = float( val )\n",
    "                break\n",
    "            except (ValueError, TypeError):\n",
    "                counter += 1\n",
    "\n",
    "        if dtype == 'datetime':\n",
    "            try:\n",
    "                tmp = pd.to_datetime( val )\n",
    "                break\n",
    "            except (ValueError, TypeError):\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Coerces dataframe column to a given data type\n",
    "########\n",
    "def change_column_type( column, dtype ):\n",
    "    if dtype in ('int', 'float'):\n",
    "        column = pd.to_numeric( column, errors='coerce' )\n",
    "        return column\n",
    "    if dtype in ('datetime'):\n",
    "        column = pd.to_datetime( column, errors='coerce' )\n",
    "        return column\n",
    "    if dtype in ('bool'):\n",
    "        column = column.map( lambda x:\n",
    "                             True if str(x).lower() in ('true','1') else\n",
    "                             False if str(x).lower() in ('false','0') else\n",
    "                             pd.NA\n",
    "                           )\n",
    "        return column\n",
    "    return column.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Creates a clean dataframe with a reduced amount of null and duplicate entries\n",
    "########\n",
    "def make_dframe( file_path, na_row_thresh=0.4, na_col_thresh=0.4 ):\n",
    "    data = read_csv_dirty( file_path )\n",
    "    lake = pd.DataFrame( data )\n",
    "    counter = 0\n",
    "\n",
    "    header_lengths = set()\n",
    "    for c_name, column in lake.items():\n",
    "        dtype = detect_column_dtype( column )\n",
    "        if dtype not in ( 'string', 'bool' ):\n",
    "            header_lengths.add( find_header( column, dtype ) )\n",
    "    if len( header_lengths ) == 0:\n",
    "        header_len = 0\n",
    "    else:\n",
    "        header_len = min( header_lengths )\n",
    "\n",
    "    if header_len == 0:\n",
    "        return lake\n",
    "\n",
    "    header = pd.MultiIndex.from_arrays( lake.iloc[:header_len].values )\n",
    "    header = [ ' '.join( map(str, col)).strip() for col in header.values ]\n",
    "\n",
    "    lake = lake.iloc[header_len:]\n",
    "    lake.columns = header\n",
    "\n",
    "    for c_name, column in lake.items():\n",
    "        dtype = detect_column_dtype( column )\n",
    "        lake[c_name] = change_column_type( column, dtype )\n",
    "\n",
    "    na_thresh = int( ( 1 - na_row_thresh ) * lake.shape[1] )\n",
    "    lake.dropna( thresh=na_thresh, inplace=True )\n",
    "    lake.drop_duplicates( inplace = True )\n",
    "\n",
    "    lake_T = lake.T\n",
    "\n",
    "    na_thresh = int( ( 1 - na_col_thresh ) * lake_T.shape[1] )\n",
    "    lake_T.dropna( thresh=na_thresh, inplace=True )\n",
    "    lake_T.drop_duplicates( inplace = True )\n",
    "\n",
    "    lake = lake_T.T\n",
    "\n",
    "    return lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Creates a set of clean dataframes\n",
    "# caches them to avoid recomputations on later executions\n",
    "########\n",
    "def cleaningData( datasets ):\n",
    "    if not os.path.exists( Path('./clean') ):\n",
    "        os.mkdir( Path('./clean') )\n",
    "\n",
    "    lakes = {}\n",
    "    for lake_name in file_names:\n",
    "        file_path = f\"./lake49/{lake_name}.csv\"\n",
    "        lake_name = f\"{lake_name}_clean\"\n",
    "        cache_path = f\"./clean/{lake_name}.csv\"\n",
    "        if os.path.exists( cache_path ):\n",
    "            with open( cache_path, 'r' ) as file:\n",
    "                lakes[lake_name] = pd.read_csv( cache_path, header=0 )\n",
    "        else:\n",
    "            lakes[lake_name] = make_dframe( file_path )\n",
    "            with open( cache_path, 'w' ) as file:\n",
    "                lakes[lake_name].to_csv( cache_path, header=True, index=False )\n",
    "\n",
    "    return lakes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16254/2163412232.py:16: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lakes[lake_name] = pd.read_csv( cache_path, header=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished shingling (104.5892ms)\n",
      "Finished onehot (29866.2550ms)\n",
      "Finished hashing (91783.6456ms)\n",
      "Finished bucketing (125.9313ms)\n",
      "\n",
      "\n",
      "====== Lazo ======\n",
      "Join Candidates: 77\n",
      "table_17_clean, table_0_clean, JOIN\n",
      "table_17_clean, table_8_clean, JOIN\n",
      "table_17_clean, table_7_clean, JOIN\n",
      "table_17_clean, table_1_clean, JOIN\n",
      "table_17_clean, table_16_clean, JOIN\n",
      "table_17_clean, table_4_clean, JOIN\n",
      "table_17_clean, table_5_clean, JOIN\n",
      "table_17_clean, table_6_clean, JOIN\n",
      "table_0_clean, table_8_clean, JOIN\n",
      "table_0_clean, table_7_clean, JOIN\n",
      "table_0_clean, table_16_clean, JOIN\n",
      "table_0_clean, table_4_clean, JOIN\n",
      "table_0_clean, table_5_clean, JOIN\n",
      "table_0_clean, table_6_clean, JOIN\n",
      "table_8_clean, table_7_clean, JOIN\n",
      "table_8_clean, table_10_clean, JOIN\n",
      "table_8_clean, table_9_clean, JOIN\n",
      "table_8_clean, table_1_clean, JOIN\n",
      "table_8_clean, table_16_clean, JOIN\n",
      "table_8_clean, table_4_clean, JOIN\n",
      "table_8_clean, table_6_clean, JOIN\n",
      "table_7_clean, table_1_clean, JOIN\n",
      "table_7_clean, table_16_clean, JOIN\n",
      "table_7_clean, table_4_clean, JOIN\n",
      "table_7_clean, table_6_clean, JOIN\n",
      "table_9_clean, table_10_clean, JOIN\n",
      "table_9_clean, table_17_clean, JOIN\n",
      "table_9_clean, table_0_clean, JOIN\n",
      "table_9_clean, table_7_clean, JOIN\n",
      "table_9_clean, table_1_clean, JOIN\n",
      "table_9_clean, table_16_clean, JOIN\n",
      "table_9_clean, table_4_clean, JOIN\n",
      "table_9_clean, table_6_clean, JOIN\n",
      "table_1_clean, table_16_clean, JOIN\n",
      "table_1_clean, table_4_clean, JOIN\n",
      "table_1_clean, table_5_clean, JOIN\n",
      "table_1_clean, table_6_clean, JOIN\n",
      "table_16_clean, table_4_clean, JOIN\n",
      "table_16_clean, table_5_clean, JOIN\n",
      "table_16_clean, table_6_clean, JOIN\n",
      "table_16_clean, table_10_clean, JOIN\n",
      "table_4_clean, table_10_clean, JOIN\n",
      "table_4_clean, table_5_clean, JOIN\n",
      "table_4_clean, table_6_clean, JOIN\n",
      "table_3_clean, table_13_clean, JOIN\n",
      "table_3_clean, table_2_clean, JOIN\n",
      "table_11_clean, table_10_clean, JOIN\n",
      "table_11_clean, table_8_clean, JOIN\n",
      "table_11_clean, table_9_clean, JOIN\n",
      "table_11_clean, table_16_clean, JOIN\n",
      "table_11_clean, table_4_clean, JOIN\n",
      "table_11_clean, table_15_clean, JOIN\n",
      "table_5_clean, table_8_clean, JOIN\n",
      "table_5_clean, table_9_clean, JOIN\n",
      "table_5_clean, table_10_clean, JOIN\n",
      "table_5_clean, table_6_clean, JOIN\n",
      "table_5_clean, table_15_clean, JOIN\n",
      "table_5_clean, table_11_clean, JOIN\n",
      "table_12_clean, table_3_clean, JOIN\n",
      "table_12_clean, table_2_clean, JOIN\n",
      "table_14_clean, table_10_clean, JOIN\n",
      "table_14_clean, table_11_clean, JOIN\n",
      "table_14_clean, table_8_clean, JOIN\n",
      "table_14_clean, table_9_clean, JOIN\n",
      "table_14_clean, table_4_clean, JOIN\n",
      "table_14_clean, table_5_clean, JOIN\n",
      "table_14_clean, table_16_clean, JOIN\n",
      "table_14_clean, table_15_clean, JOIN\n",
      "table_14_clean, table_0_clean, JOIN\n",
      "table_14_clean, table_1_clean, JOIN\n",
      "table_14_clean, table_17_clean, JOIN\n",
      "table_15_clean, table_10_clean, JOIN\n",
      "table_15_clean, table_8_clean, JOIN\n",
      "table_15_clean, table_9_clean, JOIN\n",
      "table_15_clean, table_16_clean, JOIN\n",
      "table_15_clean, table_4_clean, JOIN\n",
      "table_2_clean, table_13_clean, JOIN\n",
      "\n",
      "Union Candidates: 3\n",
      "table_0_clean, table_1_clean, UNION\n",
      "table_5_clean, table_7_clean, UNION\n",
      "table_12_clean, table_13_clean, UNION\n",
      "\n",
      "====== SilkMoth ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/pandas/core/arrays/datetimes.py:2474: RuntimeWarning: invalid value encountered in cast\n",
      "  data = data.astype(DT64NS_DTYPE).view(\"i8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join Candidates: 22\n",
      "table_17_clean, table_7_clean, JOIN\n",
      "table_17_clean, table_16_clean, JOIN\n",
      "table_17_clean, table_4_clean, JOIN\n",
      "table_17_clean, table_5_clean, JOIN\n",
      "table_17_clean, table_6_clean, JOIN\n",
      "table_0_clean, table_1_clean, JOIN\n",
      "table_0_clean, table_16_clean, JOIN\n",
      "table_0_clean, table_5_clean, JOIN\n",
      "table_0_clean, table_2_clean, JOIN\n",
      "table_13_clean, table_12_clean, JOIN\n",
      "table_7_clean, table_6_clean, JOIN\n",
      "table_10_clean, table_11_clean, JOIN\n",
      "table_10_clean, table_2_clean, JOIN\n",
      "table_1_clean, table_16_clean, JOIN\n",
      "table_1_clean, table_4_clean, JOIN\n",
      "table_1_clean, table_5_clean, JOIN\n",
      "table_1_clean, table_2_clean, JOIN\n",
      "table_16_clean, table_4_clean, JOIN\n",
      "table_16_clean, table_5_clean, JOIN\n",
      "table_4_clean, table_5_clean, JOIN\n",
      "table_3_clean, table_2_clean, JOIN\n",
      "table_11_clean, table_2_clean, JOIN\n",
      "\n",
      "Union Candidates: 12\n",
      "table_17_clean, table_3_clean, UNION\n",
      "table_0_clean, table_1_clean, UNION\n",
      "table_8_clean, table_7_clean, UNION\n",
      "table_8_clean, table_5_clean, UNION\n",
      "table_8_clean, table_6_clean, UNION\n",
      "table_13_clean, table_12_clean, UNION\n",
      "table_7_clean, table_5_clean, UNION\n",
      "table_7_clean, table_6_clean, UNION\n",
      "table_16_clean, table_2_clean, UNION\n",
      "table_4_clean, table_5_clean, UNION\n",
      "table_15_clean, table_11_clean, UNION\n",
      "table_5_clean, table_6_clean, UNION\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# Testing our newly cleaned data against our relationship detecting\n",
    "# algorithms\n",
    "########\n",
    "file_names = [ f[:-4] for f in os.listdir('./lake49') if os.path.isfile( os.path.join('./lake49', f ) ) ]\n",
    "\n",
    "lakes = cleaningData( file_names )\n",
    "\n",
    "candidates = Lazo( lakes, shingle_size=5, signature_length=100, num_bands=25, similarity_threshold=0.5, union_threshold=0.8 )\n",
    "\n",
    "print( \"\\n\\n====== Lazo ======\" )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, UNION\" )\n",
    "\n",
    "print( \"\\n====== SilkMoth ======\" )\n",
    "candidates = find_relationships( lakes, join_threshold=0.6, union_threshold=0.8 )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, UNION\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16254/2949715035.py:13: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lakes[lake_name] = pd.read_csv( file_path, delimiter=delimiter )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished shingling (142.6606ms)\n",
      "Finished onehot (40837.1022ms)\n",
      "Finished hashing (114798.7959ms)\n",
      "Finished bucketing (146.9545ms)\n",
      "\n",
      "\n",
      "====== Lazo ======\n",
      "Join Candidates: 94\n",
      "table_17_pdclean, table_0_pdclean, JOIN\n",
      "table_17_pdclean, table_8_pdclean, JOIN\n",
      "table_17_pdclean, table_7_pdclean, JOIN\n",
      "table_17_pdclean, table_10_pdclean, JOIN\n",
      "table_17_pdclean, table_9_pdclean, JOIN\n",
      "table_17_pdclean, table_1_pdclean, JOIN\n",
      "table_17_pdclean, table_16_pdclean, JOIN\n",
      "table_17_pdclean, table_4_pdclean, JOIN\n",
      "table_17_pdclean, table_5_pdclean, JOIN\n",
      "table_17_pdclean, table_6_pdclean, JOIN\n",
      "table_0_pdclean, table_8_pdclean, JOIN\n",
      "table_0_pdclean, table_7_pdclean, JOIN\n",
      "table_0_pdclean, table_10_pdclean, JOIN\n",
      "table_0_pdclean, table_9_pdclean, JOIN\n",
      "table_0_pdclean, table_16_pdclean, JOIN\n",
      "table_0_pdclean, table_4_pdclean, JOIN\n",
      "table_0_pdclean, table_5_pdclean, JOIN\n",
      "table_0_pdclean, table_6_pdclean, JOIN\n",
      "table_8_pdclean, table_7_pdclean, JOIN\n",
      "table_8_pdclean, table_10_pdclean, JOIN\n",
      "table_8_pdclean, table_9_pdclean, JOIN\n",
      "table_8_pdclean, table_1_pdclean, JOIN\n",
      "table_8_pdclean, table_16_pdclean, JOIN\n",
      "table_8_pdclean, table_4_pdclean, JOIN\n",
      "table_8_pdclean, table_5_pdclean, JOIN\n",
      "table_8_pdclean, table_6_pdclean, JOIN\n",
      "table_19_pdclean, table_18_pdclean, JOIN\n",
      "table_7_pdclean, table_1_pdclean, JOIN\n",
      "table_7_pdclean, table_16_pdclean, JOIN\n",
      "table_7_pdclean, table_4_pdclean, JOIN\n",
      "table_7_pdclean, table_6_pdclean, JOIN\n",
      "table_9_pdclean, table_10_pdclean, JOIN\n",
      "table_9_pdclean, table_7_pdclean, JOIN\n",
      "table_9_pdclean, table_1_pdclean, JOIN\n",
      "table_9_pdclean, table_16_pdclean, JOIN\n",
      "table_9_pdclean, table_4_pdclean, JOIN\n",
      "table_9_pdclean, table_6_pdclean, JOIN\n",
      "table_1_pdclean, table_10_pdclean, JOIN\n",
      "table_1_pdclean, table_16_pdclean, JOIN\n",
      "table_1_pdclean, table_4_pdclean, JOIN\n",
      "table_1_pdclean, table_6_pdclean, JOIN\n",
      "table_16_pdclean, table_10_pdclean, JOIN\n",
      "table_16_pdclean, table_4_pdclean, JOIN\n",
      "table_16_pdclean, table_6_pdclean, JOIN\n",
      "table_4_pdclean, table_10_pdclean, JOIN\n",
      "table_4_pdclean, table_6_pdclean, JOIN\n",
      "table_15_pdclean, table_10_pdclean, JOIN\n",
      "table_15_pdclean, table_17_pdclean, JOIN\n",
      "table_15_pdclean, table_0_pdclean, JOIN\n",
      "table_15_pdclean, table_8_pdclean, JOIN\n",
      "table_15_pdclean, table_9_pdclean, JOIN\n",
      "table_15_pdclean, table_1_pdclean, JOIN\n",
      "table_15_pdclean, table_16_pdclean, JOIN\n",
      "table_15_pdclean, table_4_pdclean, JOIN\n",
      "table_11_pdclean, table_10_pdclean, JOIN\n",
      "table_11_pdclean, table_0_pdclean, JOIN\n",
      "table_11_pdclean, table_17_pdclean, JOIN\n",
      "table_11_pdclean, table_8_pdclean, JOIN\n",
      "table_11_pdclean, table_9_pdclean, JOIN\n",
      "table_11_pdclean, table_1_pdclean, JOIN\n",
      "table_11_pdclean, table_16_pdclean, JOIN\n",
      "table_11_pdclean, table_4_pdclean, JOIN\n",
      "table_11_pdclean, table_15_pdclean, JOIN\n",
      "table_5_pdclean, table_10_pdclean, JOIN\n",
      "table_5_pdclean, table_9_pdclean, JOIN\n",
      "table_5_pdclean, table_1_pdclean, JOIN\n",
      "table_5_pdclean, table_16_pdclean, JOIN\n",
      "table_5_pdclean, table_4_pdclean, JOIN\n",
      "table_5_pdclean, table_15_pdclean, JOIN\n",
      "table_5_pdclean, table_7_pdclean, JOIN\n",
      "table_5_pdclean, table_6_pdclean, JOIN\n",
      "table_5_pdclean, table_11_pdclean, JOIN\n",
      "table_2_pdclean, table_0_pdclean, JOIN\n",
      "table_2_pdclean, table_8_pdclean, JOIN\n",
      "table_2_pdclean, table_9_pdclean, JOIN\n",
      "table_2_pdclean, table_1_pdclean, JOIN\n",
      "table_2_pdclean, table_16_pdclean, JOIN\n",
      "table_2_pdclean, table_4_pdclean, JOIN\n",
      "table_2_pdclean, table_5_pdclean, JOIN\n",
      "table_2_pdclean, table_17_pdclean, JOIN\n",
      "table_2_pdclean, table_13_pdclean, JOIN\n",
      "table_2_pdclean, table_12_pdclean, JOIN\n",
      "table_6_pdclean, table_10_pdclean, JOIN\n",
      "table_14_pdclean, table_10_pdclean, JOIN\n",
      "table_14_pdclean, table_11_pdclean, JOIN\n",
      "table_14_pdclean, table_17_pdclean, JOIN\n",
      "table_14_pdclean, table_0_pdclean, JOIN\n",
      "table_14_pdclean, table_8_pdclean, JOIN\n",
      "table_14_pdclean, table_9_pdclean, JOIN\n",
      "table_14_pdclean, table_1_pdclean, JOIN\n",
      "table_14_pdclean, table_16_pdclean, JOIN\n",
      "table_14_pdclean, table_4_pdclean, JOIN\n",
      "table_14_pdclean, table_15_pdclean, JOIN\n",
      "table_14_pdclean, table_5_pdclean, JOIN\n",
      "\n",
      "Union Candidates: 3\n",
      "table_0_pdclean, table_1_pdclean, UNION\n",
      "table_12_pdclean, table_13_pdclean, UNION\n",
      "table_2_pdclean, table_3_pdclean, UNION\n",
      "\n",
      "====== SilkMoth ======\n",
      "Join Candidates: 21\n",
      "table_17_pdclean, table_16_pdclean, JOIN\n",
      "table_17_pdclean, table_4_pdclean, JOIN\n",
      "table_17_pdclean, table_5_pdclean, JOIN\n",
      "table_0_pdclean, table_1_pdclean, JOIN\n",
      "table_0_pdclean, table_4_pdclean, JOIN\n",
      "table_0_pdclean, table_2_pdclean, JOIN\n",
      "table_13_pdclean, table_12_pdclean, JOIN\n",
      "table_7_pdclean, table_6_pdclean, JOIN\n",
      "table_10_pdclean, table_4_pdclean, JOIN\n",
      "table_10_pdclean, table_11_pdclean, JOIN\n",
      "table_10_pdclean, table_2_pdclean, JOIN\n",
      "table_1_pdclean, table_16_pdclean, JOIN\n",
      "table_1_pdclean, table_4_pdclean, JOIN\n",
      "table_1_pdclean, table_2_pdclean, JOIN\n",
      "table_16_pdclean, table_4_pdclean, JOIN\n",
      "table_16_pdclean, table_5_pdclean, JOIN\n",
      "table_4_pdclean, table_11_pdclean, JOIN\n",
      "table_4_pdclean, table_5_pdclean, JOIN\n",
      "table_4_pdclean, table_2_pdclean, JOIN\n",
      "table_3_pdclean, table_2_pdclean, JOIN\n",
      "table_11_pdclean, table_2_pdclean, JOIN\n",
      "\n",
      "Union Candidates: 9\n",
      "table_0_pdclean, table_1_pdclean, UNION\n",
      "table_8_pdclean, table_7_pdclean, UNION\n",
      "table_8_pdclean, table_5_pdclean, UNION\n",
      "table_8_pdclean, table_6_pdclean, UNION\n",
      "table_13_pdclean, table_12_pdclean, UNION\n",
      "table_7_pdclean, table_5_pdclean, UNION\n",
      "table_4_pdclean, table_5_pdclean, UNION\n",
      "table_15_pdclean, table_11_pdclean, UNION\n",
      "table_5_pdclean, table_6_pdclean, UNION\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########\n",
    "# Sanity check where tables are simply read with pandas\n",
    "# and cleaning is left to default pandas behavior\n",
    "# (column dtype detection, single-line header detection, etc.)\n",
    "########\n",
    "file_names = [ f[:-4] for f in os.listdir('./lake49') if os.path.isfile( os.path.join('./lake49', f ) ) ]\n",
    "\n",
    "lakes = {}\n",
    "for lake_name in file_names:\n",
    "    file_path = f\"./lake49/{lake_name}.csv\"\n",
    "    delimiter = find_delimiter( file_path )\n",
    "    lake_name = lake_name + '_pdclean'\n",
    "    lakes[lake_name] = pd.read_csv( file_path, delimiter=delimiter )\n",
    "\n",
    "candidates = Lazo( lakes, shingle_size=5, signature_length=100, num_bands=25, similarity_threshold=0.5, union_threshold=0.8 )\n",
    "\n",
    "print( \"\\n\\n====== Lazo ======\" )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate[0]}, {candidate[1]}, UNION\" )\n",
    "\n",
    "\n",
    "print( \"\\n====== SilkMoth ======\" )\n",
    "candidates = find_relationships( lakes, join_threshold=0.6, union_threshold=0.8 )\n",
    "print( f\"Join Candidates: {len(candidates['join'])}\" )\n",
    "for candidate in candidates['join']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, JOIN\" )\n",
    "\n",
    "print( f\"\\nUnion Candidates: {len(candidates['union'])}\" )\n",
    "for candidate in candidates['union']:\n",
    "    print( f\"{candidate['table1']}, {candidate['table2']}, UNION\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aspects of the data that most influenced our data discovery process are defined below:\n",
    "\n",
    "1. different deliminators\n",
    "The CSV files in our data lake used different delimiters. We fixed this by writing a delimiter discovery function. This function analyzes both the number of non-alphanumeric characters in each row of the table and the variance of these counts across rows. The character with the highest ratio of frequency to variance (+ a small constant ðœ–) emerges as the most likely delimiter. This approach successfully fixed this issue.\n",
    "\n",
    "2. Data tables with different length rows\n",
    "When some rows in a dataset have different lengths than others, comparison of datasets becomes difficlt. To address this, NaN values are added to the shorter rows to fill in the missing data and match the length of the longest row.\n",
    "\n",
    "3. columns of all zeroes\n",
    "When rows contain only zeros, any comparison function treats them as identical, even though they likely represent different data. Therefore, these rows are ignored.\n",
    "nulls nans\n",
    "\n",
    "4. Large size of data\n",
    "Another problem we encountered was the large size of out datasets, especially table 14 was large. To still be able to do data discovery we used the computationally efficient Lazo as well as Silkmoth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
